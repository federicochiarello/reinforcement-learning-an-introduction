{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Homework\n",
    "\n",
    "Before you turn this notebook in, make sure everything runs as expected. First, **restart the kernel** (in the menu bar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menu bar, select Cell $\\rightarrow$ Run All). You can also perform the two operations in one step (in the menu bar, select Kernel $\\rightarrow$ Restart & Run All).\n",
    "\n",
    "Make sure you fill in any place (or anywhere that you want in the code) that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", and remove every line containing the expression: \"raise ...\" (if you leave such a line your code will not run). This is a reminder for you to complete all parts of the notebook. You can also send an incomplete notebook, but please remove all 'raise'-lines in any case.\n",
    "\n",
    "You are not required to modify the code where the line `YOUR CODE HERE` does not appear. In fact you can modify the given function and code as you want, at your own risk! Those are just reminders.\n",
    "\n",
    "Do not remove any cell from the notebook you downloaded. You can add any number of cells (and remove them if not more necessary). \n",
    "\n",
    "Do not use different packages from the ones that are already imported! (the evaluation may fail otherwise)\n",
    "\n",
    "Please use only google colab or jupyter to edit this notebook! Other software may save notebooks in a format that may interfere with the evaluation procedure.\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your code, since this is the way we will do it before evaluating your notebook!\n",
    "\n",
    "Fill in your name and ID number (matricola) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = ''\n",
    "ID_number = int('')\n",
    "\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92f4555e2f4a0e4f0ff89168cb1b4cdd",
     "grade": false,
     "grade_id": "cell-ad7c80aa60296d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework II - Reinforcement Learning with Jackblack\n",
    "\n",
    "The following homework allows you to review many concepts from the course of Reinforcement Learning.\n",
    "\n",
    "In particular you will have to implement Monte Carlo algorithm, SARSA-$\\lambda$ and SARSA-$\\lambda$ with action-value function approximation. In order to test the implementations, we need an environment. Therefore you need also to complete the given code for a modified version of Blackjack game seen in class, invented by Jack from the car rental. In a burst of creativity, he called it **Jackblack** (with no reference to the actor).\n",
    "\n",
    "Please, use google colab or jupyter to complete the notebook! Also be sure to deliver the notebook with the correct format (.ipynb). Corrupted notebooks or notebooks with cells missing may not be considered eligible (pun intended) for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d693403b4af36baf74518c688a0f5ab",
     "grade": false,
     "grade_id": "cell-eedca95cee20f94d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Needed imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T08:45:43.355832Z",
     "start_time": "2020-08-01T08:45:37.362298Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "beed9400206852ee9561eca92b318965",
     "grade": false,
     "grade_id": "cell-5731dd0189b7307d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from copy import deepcopy\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display\n",
    "\n",
    "random.seed(ID_number)\n",
    "np.random.seed(ID_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef604be2a2e3bb6c0216073aea03cea3",
     "grade": false,
     "grade_id": "cell-73e6b86e5b3d321f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Jackblack rules:\n",
    "\n",
    "- The game is played with an infinite deck of cards (i.e. cards are sampled\n",
    "with replacement)\n",
    "\n",
    "\n",
    "- Each draw from the deck results in a value between 1 and 10 (uniformly\n",
    "distributed) with a colour of _red_ (probability 1/3) or _black_ (probability\n",
    "2/3).\n",
    "\n",
    "\n",
    "- There are no aces or picture (face) cards in this game (no issues with usable aces!)\n",
    "\n",
    "\n",
    "- At the start of the game both the player and the dealer draw one black\n",
    "card (fully observed)\n",
    "\n",
    "\n",
    "- Each turn the player may either **stick** or **hit**:\n",
    "    - If the player hits then he draws another card from the deck\n",
    "    - If the player sticks he receives no further cards and its turn ends\n",
    "\n",
    "\n",
    "- The values of the player’s cards are added (if black card) or subtracted (if red\n",
    "card)\n",
    "\n",
    "\n",
    "- If the player’s sum exceeds 21, or becomes less than 1, then he “goes\n",
    "bust” and loses the game, with a reward of -1\n",
    "\n",
    "\n",
    "- If the player sticks then the dealer starts taking turns, same rules apply to him. The dealer always\n",
    "sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes\n",
    "bust, then the player wins, with a reward of 1; otherwise, the outcome – _win_ (reward 1),\n",
    "_lose_ (reward -1), or _draw_ (reward 0) – is given by the player with the largest sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87983c38272e6dc32e971f8cc128f008",
     "grade": false,
     "grade_id": "cell-a959d22d3a7a11d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implementation of Jackblack environment\n",
    "\n",
    "Follow the instructions givenn in each function and implement the missing ones so that the environment will work as expected.\n",
    "\n",
    "As it usually happens with games, where we receive only a signle reward at the end of the episode, we will assume that there is no discounting, i.e. $\\gamma=1$. This will hold throughout the entire notebook (hence you will not see that parameter)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T08:45:47.184467Z",
     "start_time": "2020-08-01T08:45:47.067779Z"
    },
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "436586a1a2ccd698612a1c730399d0f5",
     "grade": false,
     "grade_id": "cell-e4b65a334a17db23",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Jackblack():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Init the environment with its inner variables.\n",
    "        The first state by picking a random card for the dealer and player.\n",
    "        \"\"\"\n",
    "        dealer_score, _ = self.draw_card()\n",
    "        player_score, _ = self.draw_card()\n",
    "        self.state = {\"dealer_score\": dealer_score, \"player_score\": player_score}  # initial state\n",
    "        self.actions = (\"hit\", \"stick\")\n",
    "               \n",
    "        init_state = self.state.copy()\n",
    "        self.history = [init_state]  # game history, recording state and action of each step\n",
    "        \n",
    "        \n",
    "    def step(self, action, verbose=False):\n",
    "        \"\"\"\n",
    "        Compute a step in Easy21 game. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : string, the action to pick\n",
    "        verbose : bool, if True actions are printed\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state : dict, the new state reached given the picked action\n",
    "        reward : int, the reward we get in this new state\n",
    "        done : bool, True if state is terminal, False if not\n",
    "        \"\"\"\n",
    "        self.history.append({\"player\": action})\n",
    "        \n",
    "        # player hits\n",
    "        if action == \"hit\":\n",
    "            value, colour = self.draw_card()\n",
    "            self.state['player_score'] = self.compute_new_score(value, colour, self.state['player_score'])\n",
    "            if verbose:\n",
    "                print(f'Player draws {colour} {value}')\n",
    "            \n",
    "            new_state = self.state.copy()\n",
    "            self.history.append(new_state)\n",
    "            \n",
    "            if self.goes_bust(self.state['player_score']):\n",
    "                # player goes bust\n",
    "                reward = -1                \n",
    "                if verbose:\n",
    "                    print('Player goes bust')\n",
    "                    print('Player loses')\n",
    "                return self.state, reward, True\n",
    "            \n",
    "            else:\n",
    "                reward = 0\n",
    "                return self.state, reward, False\n",
    "            \n",
    "        # player sticks   \n",
    "        else:\n",
    "            if verbose:\n",
    "                    print(f'Player stops')\n",
    "            new_state = self.state.copy()\n",
    "            self.history.append(new_state)\n",
    "            \n",
    "            state, reward = self.dealer_moves(verbose)\n",
    "            return self.state, reward, True\n",
    "\n",
    "        \n",
    "    def draw_card(self): # TODO1\n",
    "        \"\"\"\n",
    "        Each draw from the deck results in a value between 1 and 10 (uniformly\n",
    "        distributed) with a colour of red (probability 1/3) or black (probability 2/3).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value : int, the value of the card\n",
    "        colour : string, the colour of the card, \"red\" or \"black\"\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def goes_bust(self, score): # TODO2\n",
    "        \"\"\"\n",
    "        Check if the player/dealer went bust\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        score : int, the score to check\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool : True if score is outside the prescribed range\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def compute_new_score(self, value, colour, current_score): # TODO3\n",
    "        \"\"\"\n",
    "        Compute the new score given the value and the colour of the drawn card.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value : int, the card's value\n",
    "        colour : string, the card's colour\n",
    "        current_score : int, the score to update\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_score : int, updated score\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def dealer_moves(self, verbose=False): \n",
    "        \"\"\"\n",
    "        Fixed dealer policy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : bool, if True actions are printed\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state : state, the terminal state of the whole game sequence\n",
    "        reward : int, the reward obtained at the end of the game sequence\n",
    "        \"\"\"\n",
    "        # dealer hits as long as his score is < 17\n",
    "        while self.state['dealer_score'] < 17:\n",
    "            value, colour = self.draw_card()\n",
    "            self.state['dealer_score'] = self.compute_new_score(value, colour, self.state['dealer_score'])\n",
    "            if verbose:\n",
    "                print(f'Dealer draws {colour} {value}')\n",
    "            \n",
    "            new_state = self.state.copy()\n",
    "            self.history.append({\"dealer\": \"hit\"})\n",
    "            self.history.append(new_state)\n",
    "            \n",
    "            \n",
    "            if self.goes_bust(self.state['dealer_score']):\n",
    "                # dealer goes bust, player wins\n",
    "                if verbose:\n",
    "                    print('Dealer goes bust')\n",
    "                    print('Dealer loses')\n",
    "                reward = 1\n",
    "                return self.state, reward\n",
    "            \n",
    "        self.history.append({\"dealer\": \"stick\"})  \n",
    "        \n",
    "        player_score = self.state['player_score']\n",
    "        dealer_score = self.state['dealer_score']  \n",
    "        \n",
    "        # score > 17 -> dealer sticks\n",
    "        if verbose:\n",
    "            print('Dealer stops')\n",
    "        if dealer_score < player_score: # player wins\n",
    "            reward = 1\n",
    "            if verbose:\n",
    "                print('Player wins')\n",
    "            return self.state, reward                    \n",
    "        if dealer_score == player_score: # draw\n",
    "            reward = 0\n",
    "            if verbose:\n",
    "                print('Draw')\n",
    "            return self.state, reward                 \n",
    "        if dealer_score > player_score: # player loses\n",
    "            reward = -1\n",
    "            if verbose:\n",
    "                print('Dealer wins')\n",
    "            return self.state, reward\n",
    "        \n",
    "        \n",
    "    def print_score(self):\n",
    "        \"\"\"\n",
    "        Print the current score of the game, both for player and dealer.\n",
    "        \"\"\"\n",
    "        print('----------------')\n",
    "        for player in self.state.items():\n",
    "            print(player[0] + ': ' + str(player[1]))\n",
    "        print('----------------')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13e606287d653e90f36777f3adcfa246",
     "grade": true,
     "grade_id": "cell-bcf070211195e6b2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AT THIS POINT THE ENVIRONMENT MUST BE COMPLETED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3afe6df8efb69cef554cf401160a7d49",
     "grade": false,
     "grade_id": "cell-f1543b166790d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now explore some steps of a Jackblack game to check if the environment is coherent with the given rules. You can run the next cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T09:38:44.308886Z",
     "start_time": "2020-06-21T09:38:44.300522Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eaa3203aa49ed1ab3300e63c4eeed56",
     "grade": false,
     "grade_id": "cell-0b90c33c38eeb33f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = Jackblack()\n",
    "env.print_score()\n",
    "env.step(\"hit\", True)\n",
    "env.print_score()\n",
    "env.step(\"hit\", True)\n",
    "env.print_score()\n",
    "env.step(\"stick\", True)\n",
    "env.print_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f095c918731167f0d1d3a2a4c3946d0",
     "grade": false,
     "grade_id": "cell-2fd4fc5eb7d67672",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Monte-Carlo Control of Jackblack\n",
    "\n",
    "Let's implement the classical version of Monte Carlo algorithm for action-value functions in order to find a good policy to win over the dealer.\n",
    "\n",
    "The $\\epsilon$-greedy policy should be implemented such that: \n",
    "$$ \\epsilon = \\frac{N_0}{N_0 + c_1}  $$\n",
    "\n",
    "where $c_1$ is the number of times the state has been encountered so far. You can (should!) try to understand how the parameter $N_0$ works to set a meaningful trade-off between exploration and exploitation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T08:45:57.405782Z",
     "start_time": "2020-08-01T08:45:57.390244Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e68a298001b297cd8faa4baaffe8e22",
     "grade": false,
     "grade_id": "cell-10f70869a3d2b6b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MC_Control():\n",
    "    \n",
    "    \n",
    "    def __init__(self, N_0=100, n_episodes=100, lr=None):\n",
    "        \"\"\"\n",
    "        Init the Monte Carlo control class with its inner variables.\n",
    "        Try to understand how the N_0 parameter works!\n",
    "        Both the Q-table and the counter are initialized to zero.\n",
    "        The first episode is played with a random policy.\n",
    "        \"\"\"\n",
    "        self.actions = (\"hit\", \"stick\") \n",
    "        self.N_0 = N_0                  # constant parameter (influence the exploration/exploitation behavior!)\n",
    "        self.n_episodes = n_episodes    # number of episodes (games) to sample in order to make the agent learn\n",
    "        self.lr = lr                    # The learning rate: \"how much\" the Q-function is updated at each step\n",
    "        \n",
    "        self.Q = self.init_to_zeros()   # init Q function to zeros\n",
    "        self.N = self.init_to_zeros()   # init N to zeros\n",
    "        self.policy = \"random\"          # arbitrarily init the MC learning with a random policy\n",
    " \n",
    "\n",
    "    def learn_q_value_function(self): # TODO4 (see other functions to understand how it should be done!)\n",
    "        \"\"\"\n",
    "        Update the Q-function over all the episodes.\n",
    "        From the second episode onwards you should use the epsilon greedy policy: see code below in order to\n",
    "        understand how to do that!\n",
    "        Pay attention to the <play_episode> function to understand how to implement this one!\n",
    "        Do not forget to update both the Q-table and the counter.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        Q : dictionary of dictionaries. Every state is a key, whose value is another dictionary. The latter\n",
    "        dictionary's keys are the actions, and the values are the Q-values for the corresponding state-action pair.\n",
    "        {state: (action)}, Q-value for every state-action pair\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def init_to_zeros(self):\n",
    "        \"\"\"\n",
    "        Used to init the Q-function and the incremental counter N at 0 for every state-action pairs.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        lookup_table : {state: (action)}, a dictionary of states as keys and dictionaries as value, in which the actions\n",
    "        are keys and corresponding Q-values are values\n",
    "        \"\"\"\n",
    "        dealer_scores = np.arange(1, 11)\n",
    "        player_scores = np.arange(1, 22)\n",
    "        states = [(dealer_score, player_score) for player_score in player_scores for dealer_score in dealer_scores]       \n",
    "        lookup_table = {}\n",
    "        for state in states:\n",
    "            lookup_table[state] = {\"hit\": 0, \"stick\": 0}  # initialize to 0 the Q-values!\n",
    "            \n",
    "        return lookup_table\n",
    "        \n",
    "    \n",
    "    def play_episode(self):\n",
    "        \"\"\"\n",
    "        Run a complete (from the initial state to the terminal state) Jackblack game sequence following the given policy. \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        episode : [(state, action, reward, done)], a list of (state, action, reward, done)\n",
    "        \"\"\"\n",
    "        env = Jackblack()                # init a game sequence\n",
    "        state = env.state.copy()         # init state\n",
    "        episode = []                     # list of the steps of the game sequence\n",
    "        done = False\n",
    "        while not done:      \n",
    "            # pick an action regarding the current state and policy\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done = deepcopy(env.step(action))\n",
    "            step = (state, action, reward, done)  # store previous state\n",
    "            state = next_state\n",
    "            episode.append(step)\n",
    "            \n",
    "        return episode\n",
    "\n",
    "\n",
    "    def update_Q(self, state, action, total_reward):  # TODO5\n",
    "        \"\"\"\n",
    "        Update Q value towards the target.\n",
    "        Even if this function does not return anything, you need to update <self.Q>!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, the current state\n",
    "        action : string, the current action\n",
    "        reward : int, the reward for that state-action pair\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        \n",
    "        # The learning rate, decaying along with the number of times an action-state pair \n",
    "        # has been explored. It represents the amount of modification we want to bring to \n",
    "        # the Q value function.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "    def increment_counter(self, state, action):\n",
    "        \"\"\"\n",
    "        Increment N counter for every action-state pair encountered in an episode.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, the current state\n",
    "        action : string, the current action\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        self.N[lookup_state][action] += 1\n",
    "    \n",
    "    \n",
    "    def random_policy(self):  # TODO6\n",
    "        \"\"\"\n",
    "        Return an action following a random policy (clearly state free).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, random action\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "     \n",
    "     \n",
    "    def e_greedy_policy(self, state):  # TODO7\n",
    "        \"\"\"\n",
    "        Return an action given by an epsilon greedy policy. Check all the given functions (also below)!\n",
    "        They can be of great help!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state where we pick the action\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, action from epsilon greedy policy\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def get_action(self, state):  # TODO8\n",
    "        \"\"\"\n",
    "        Return an action following the policy prescribed by the inner value of the class (either \"random\" or \"e_greedy\").\n",
    "        Use the functions above!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state where we pick the action\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, action from prescribed policy\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def get_action_w_max_value(self, state):\n",
    "        \"\"\"\n",
    "        Return the action with the max Q-value at a given state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state for the Q-function\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, best action for the state according to Q-function\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        list_values = list(self.Q[lookup_state].values())\n",
    "        if list_values[0] == list_values[1]:\n",
    "            return self.random_policy()      # randomly breaking ties\n",
    "        else:\n",
    "            action = max(self.Q[lookup_state], key=self.Q[lookup_state].get) \n",
    "            return action\n",
    "    \n",
    "    \n",
    "    def get_state_counter(self, state):\n",
    "        \"\"\"\n",
    "        Return the counter for a given state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict or tuple with values\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        counter : int, the number of times a state has been explored\n",
    "        \"\"\"\n",
    "        try:\n",
    "            counter = np.sum(list(self.N[state].values()))\n",
    "        except TypeError:\n",
    "            lookup_state = tuple(state.values())\n",
    "            counter = np.sum(list(self.N[lookup_state].values()))\n",
    "        \n",
    "        return counter\n",
    "    \n",
    "    \n",
    "    def get_state_action_counter(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the counter for a given action-state pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict or tuple with values\n",
    "        action : string\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        counter : int, the number of times an action-state pair has been explored\n",
    "        \"\"\"\n",
    "        try:\n",
    "            counter = self.N[state][action]\n",
    "        except TypeError:\n",
    "            lookup_state = tuple(state.values())\n",
    "            counter = self.N[lookup_state][action]\n",
    "        \n",
    "        return counter\n",
    "    \n",
    "\n",
    "    def optimal_policy(self):\n",
    "        \"\"\"\n",
    "        Return the learned value-function and a dataframe with the optimal action for each state.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        max_Q : nd-array, the value-function\n",
    "        df_pivot : dataframe, representing the best action for each state\n",
    "        \"\"\"\n",
    "        max_Q = np.ndarray(shape=(21, 10))\n",
    "        df = pd.DataFrame(columns=[\"dealer_showing\", \"player_score\", \"best_action\"])\n",
    "        states = list(self.Q.keys())\n",
    "        for i in range(len(states)):\n",
    "            best_action = max(self.Q[states[i]], key=self.Q[states[i]].get)\n",
    "            max_Q[states[i][1]-1][states[i][0]-1] = max(self.Q[states[i]].values())\n",
    "            df.loc[i] = (states[i][0], states[i][1], best_action)\n",
    "        df_pivot = df.pivot(\"player_score\", \"dealer_showing\", \"best_action\")\n",
    "        \n",
    "        return max_Q, df_pivot\n",
    "    \n",
    "    \n",
    "def plot_table(table):\n",
    "    \"\"\"\n",
    "    Plot the given value function in 3D.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table : nd-array, the value-function to be plotted\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    dealer_showing = np.arange(1, 11)\n",
    "    player_score = np.arange(1, 22)\n",
    "    dealer_showing, player_score = np.meshgrid(dealer_showing, player_score)\n",
    "    # Plot the surface.\n",
    "    surf = ax.plot_surface(dealer_showing, player_score, table, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "    # Customize plot\n",
    "    ax.set_zlim(-1.01, 1.01)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "    plt.xlabel('Dealer showing', fontsize=12)\n",
    "    plt.ylabel('Player score', fontsize=12)\n",
    "    plt.title('Value function', fontsize=16)\n",
    "    plt.xticks(np.arange(1, 11))\n",
    "    plt.yticks(np.arange(1, 22))\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14db0906cc702d52425ad7783de33677",
     "grade": true,
     "grade_id": "cell-32c105159fd9d573",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AT THIS POINT THE MONTE CARLO CLASS MUST BE COMPLETED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "340e75d4fd55f347a9c74acbacd6674a",
     "grade": false,
     "grade_id": "cell-76c792513cdf3280",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's play 50_000 episodes and learn the Q-function with the Monte Carlo algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T12:15:44.871669Z",
     "start_time": "2020-05-15T12:15:44.866867Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dc49684138ad9a083ef0f79aa576303",
     "grade": false,
     "grade_id": "cell-1d0580a4596c334d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mc = MC_Control(n_episodes=50_000)\n",
    "mc.learn_q_value_function();\n",
    "mcvalue, mcpol = mc.optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd2dbfdf418615d71b70d4785b43c49f",
     "grade": false,
     "grade_id": "cell-83eccb1ce5ea36f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Display the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffc4ec06ce8d133ac9b5e3f5665a5429",
     "grade": false,
     "grade_id": "cell-046d3a1485fddf41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "display(mcpol)\n",
    "plot_table(mcvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ac297990aa1c5241b12c081469e0fd3",
     "grade": false,
     "grade_id": "cell-09bf340065d5e329",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TD learning with SARSA(λ) in Jackblack\n",
    "\n",
    "You shall now implement the SARSA(λ) algorithm, also using eligibility traces (both accumulating and replacing). Do you expect it will learn a better value function than Monte Carlo? Let's see. For the code below we can re-use some of the functions from the Monte Carlo implementation (but you should pay attention!).\n",
    "\n",
    "The function should be implemented such that: \n",
    "$$ \\alpha = \\frac{1}{c_2 + 1} $$\n",
    "\n",
    "where $c_2$ is the number of times that state-action pair has been encountered so far, and alpha is the learning rate that multiplies the TD error in the update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T08:49:57.476795Z",
     "start_time": "2020-08-01T08:49:57.449670Z"
    },
    "code_folding": [
     55,
     141,
     173
    ],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dde1d5e0c9959fe56afd9cb74b774922",
     "grade": false,
     "grade_id": "cell-16fac3e58bbdf411",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SARSA():\n",
    "    \n",
    "    \n",
    "    def __init__(self, lamb=.9, N_0=100, n_episodes=100, trace=\"accumulating\"):\n",
    "        \"\"\"\n",
    "        Init the SARSA class with its inner variables.\n",
    "        Both the Q-table and the counter are initialized to zero.\n",
    "        \"\"\"\n",
    "        self.actions = (\"hit\", \"stick\") \n",
    "        self.lamb = lamb                # lambda parameter of the SARSA algorithm\n",
    "        self.n_episodes = n_episodes    # number of episodes (games) to sample in order to make the agent learn\n",
    "        self.N_0 = N_0                  # constant parameter (influence the exploration/exploitation behavior!)\n",
    "        self.trace = trace\n",
    "        \n",
    "        self.Q = self.init_to_zeros()                    # init Q function to zeros\n",
    "        self.N = self.init_to_zeros()                    # init the counter traces to zeros\n",
    "        self.eligibilty_traces = self.init_to_zeros()    # init eligibilty traces to zeros\n",
    "\n",
    "        \n",
    "    def learn_q_value_function(self):\n",
    "        \"\"\"\n",
    "        Update the Q-function over all the episodes.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        Q : dictionary of dictionaries. Every state is a key, whose value is another dictionary. The latter\n",
    "        dictionary's keys are the actions, and the values are the Q-values for the corresponding state-action pair.\n",
    "        {state: (action)}, Q-value for every state-action pair\n",
    "        \"\"\"\n",
    "        for i in range(self.n_episodes):\n",
    "            self.eligibilty_traces = self.init_to_zeros()    # init eligibilty traces to zeros\n",
    "            env = Jackblack()                                # init a game sequence\n",
    "            state = env.state.copy()                         # init state    \n",
    "            action = self.e_greedy_policy(state)             # pick a first action\n",
    "            self.increment_counter(state, action)\n",
    "            \n",
    "            done = False\n",
    "            while not done:      \n",
    "                next_state, reward, done = deepcopy(env.step(action))\n",
    "                \n",
    "                if done:\n",
    "                    next_action = None\n",
    "                    delta = self.compute_delta(state, action, next_state, next_action, reward)\n",
    "                    \n",
    "                else:   \n",
    "                    next_action = self.e_greedy_policy(next_state)   \n",
    "                    delta = self.compute_delta(state, action, next_state, next_action, reward)\n",
    "                    self.increment_counter(next_state, next_action)\n",
    "                                                \n",
    "                self.increment_eligibility_traces(state, action)\n",
    "                self.update_step(delta)\n",
    "                \n",
    "                action = next_action\n",
    "                state = next_state\n",
    "            \n",
    "        return self.Q \n",
    "\n",
    "    \n",
    "    def init_to_zeros(self):\n",
    "        \"\"\"\n",
    "        Used to init the Q-function and the incremental counter N at 0 for every state-action pairs.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        lookup_table : {state: (action)}, a dictionary of states as keys and dictionaries as value, in which the actions\n",
    "        are keys and corresponding Q-values are values\n",
    "        \"\"\"\n",
    "        dealer_scores = np.arange(1, 11)\n",
    "        player_scores = np.arange(1, 22)\n",
    "        states = [(dealer_score, player_score) for player_score in player_scores for dealer_score in dealer_scores]       \n",
    "        lookup_table = {}\n",
    "        for state in states:\n",
    "            lookup_table[state] = {\"hit\": 0, \"stick\": 0}  \n",
    "            \n",
    "        return lookup_table\n",
    "\n",
    "    \n",
    "    def update_step(self, delta):  # TODO9\n",
    "        \"\"\"\n",
    "        Update the Q value towards the target, and also the eligibility traces.\n",
    "        Implement the correct procedure to update the Q-function and the eligibility traces, with given delta.\n",
    "        Recall that here traces have already been accumulated/replaced, so there is no need to take care of that.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : float, the TD error for the current state-action pair\n",
    "        \"\"\"\n",
    "        # Here is where the lambda parameter intervenes. The higher, the slower the eligibility trace\n",
    "        # associated to a state-action will fade away.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def compute_delta(self, state, action, next_state, next_action, reward):\n",
    "        \"\"\"\n",
    "        Compute delta, the TD error.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, the current state\n",
    "        action : string, the current action\n",
    "        reward : int, the reward for the current state-action pair\n",
    "        next_state : dict, the state we end up in after performing the action\n",
    "        next_action : string, the action we take in next state following the (e_greedy) policy\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        delta : float, the TD error\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        if next_action is None:\n",
    "            delta = reward - self.Q[lookup_state][action]\n",
    "        else:\n",
    "            next_lookup_state = tuple(next_state.values())\n",
    "            delta = reward + self.Q[next_lookup_state][next_action] - self.Q[lookup_state][action]\n",
    "        return delta\n",
    "    \n",
    "    \n",
    "    def increment_eligibility_traces(self, state, action):  # TODO10\n",
    "        \"\"\"\n",
    "        Replace or accumulate the active trace.\n",
    "        To choose between accumulating or replacing traces, query the corresponding inner variable of the class,\n",
    "        and implement the update accordingly.\n",
    "        You just need to accumulate or replace, the \"fading\" is taken into account in <update_step>!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, active state\n",
    "        action : string, active action\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "    def increment_counter(self, state, action):\n",
    "        \"\"\"\n",
    "        Increment N counter for every action-state pair encountered in an episode.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : state, the current score\n",
    "        action : string, the current score\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        self.N[lookup_state][action] += 1\n",
    "        \n",
    "    \n",
    "    def random_policy(self):  # TODO11\n",
    "        \"\"\"\n",
    "        Return an action following a random policy (clearly state free).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, random action\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def e_greedy_policy(self, state):  # TODO12\n",
    "        \"\"\"\n",
    "        Return an action given an epsilon greedy policy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state where we pick the action\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, action from epsilon greedy policy\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def get_action_w_max_value(self, state):\n",
    "        \"\"\"\n",
    "        Return the action with the max Q-value at a given state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state for the Q-function\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, best action for the state according to Q-function\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        list_values = list(self.Q[lookup_state].values())\n",
    "        if list_values[0] == list_values[1]:\n",
    "            return self.random_policy()\n",
    "        else:\n",
    "            action = max(self.Q[lookup_state], key=self.Q[lookup_state].get) \n",
    "            return action\n",
    "\n",
    "        \n",
    "    def get_state_counter(self, state):\n",
    "        \"\"\"\n",
    "        Return the counter for a given state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict or tuple with values\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        counter : int, the number of times a state has been explored\n",
    "        \"\"\"\n",
    "        try:\n",
    "            counter = np.sum(list(self.N[state].values()))\n",
    "        except TypeError:\n",
    "            lookup_state = tuple(state.values())\n",
    "            counter = np.sum(list(self.N[lookup_state].values()))\n",
    "        \n",
    "        return counter\n",
    "\n",
    "    \n",
    "    def get_state_action_counter(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the counter for a given action-state pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict or tuple with values\n",
    "        action : string\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        counter : int, the number of times an action-state pair has been explored\n",
    "        \"\"\"\n",
    "        try:\n",
    "            counter = self.N[state][action]\n",
    "        except TypeError:\n",
    "            lookup_state = tuple(state.values())\n",
    "            counter = self.N[lookup_state][action]\n",
    "        \n",
    "        return counter     \n",
    "    \n",
    "    \n",
    "    def optimal_policy(self):\n",
    "        \"\"\"\n",
    "        Return the learned value-function and a dataframe with the optimal action for each state.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        max_Q : nd-array, the value-function\n",
    "        df_pivot : dataframe, representing the best action for each state\n",
    "        \"\"\"\n",
    "        max_Q = np.ndarray(shape=(21, 10))\n",
    "        df = pd.DataFrame(columns=[\"dealer_showing\", \"player_score\", \"best_action\"])\n",
    "        states = list(self.Q.keys())\n",
    "        for i in range(len(states)):\n",
    "            best_action = max(self.Q[states[i]], key=self.Q[states[i]].get)\n",
    "            max_Q[states[i][1]-1][states[i][0]-1] = max(self.Q[states[i]].values())\n",
    "            df.loc[i] = (states[i][0], states[i][1], best_action)\n",
    "        df_pivot = df.pivot(\"player_score\", \"dealer_showing\", \"best_action\")\n",
    "        \n",
    "        return max_Q, df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec7942512a21ca2723d0a764b3bae8e",
     "grade": true,
     "grade_id": "cell-db119bab4165774d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AT THIS POINT THE SARSA CLASS MUST BE COMPLETED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5fd5f0271e5ccf93ccb15ad4c1dfb75",
     "grade": false,
     "grade_id": "cell-4844dfa97df07d51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's play 50_000 episodes and learn the Q-function with the SARSA algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T21:30:00.777776Z",
     "start_time": "2020-06-16T21:30:00.576591Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc8973fbef3a68a9460c310637ea26a5",
     "grade": false,
     "grade_id": "cell-c32b246c7c4423d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sarsa = SARSA(n_episodes=50_000)\n",
    "sarsa.learn_q_value_function();\n",
    "sarsavalue, sarsapol = sarsa.optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4aeb4ebb140128373fdaaf6e7e3d8cb4",
     "grade": false,
     "grade_id": "cell-a3b74989d6dfec41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Display the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1eedea3dce337f2c540f451f374f986",
     "grade": false,
     "grade_id": "cell-072913bcf24ec1f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "display(sarsapol)\n",
    "plot_table(sarsavalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a6f969034ecc24915ba19fa89a59771",
     "grade": false,
     "grade_id": "cell-c2b52e2d9816a0f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Value Approximation in Jackblack\n",
    "\n",
    "Even if our environment is not that complicated, and has a very limited number of states and actions, introducing a linear approximation for the Q-function may be beneficial, especially if we restrict the number of episode to 50_000 (which is not that high).\n",
    "\n",
    "In the code below you shall implement the linear value approximation of the SARSA(λ) algorithm, where features are overlapped! In particular, the tile coding for the player's sum is made of 6 features; each feature is active if the state lies in between the 2 values (extremes included):\n",
    "\n",
    "$(1, 6), \\ (4, 9), \\ (7, 12), \\ (10, 15), \\ (13, 18), \\ (16, 21)$\n",
    "\n",
    "As you see, there are some states in which there is more than one active feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T09:07:10.821327Z",
     "start_time": "2020-08-01T09:07:10.799642Z"
    },
    "code_folding": [
     68,
     127,
     175
    ],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1201b754f0ab6f379e22270463593748",
     "grade": false,
     "grade_id": "cell-b3279e5254b7b32f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Value_Approximation():\n",
    "    \n",
    "    \n",
    "    def __init__(self, lamb=.9, epsilon=0.05, n_episodes=100, lr=0.01, trace=\"accumulating\"):\n",
    "        \"\"\"\n",
    "        Init the SARSA approximation class with its inner variables.\n",
    "        Features are created with the given function.\n",
    "        Here the epsilon and learning rate parameters are given explicitly and fixed.\n",
    "        Initial weights for the linear function approximation are sampled from a Gaussian.\n",
    "        Both the Q-table and the counter are initialized to zero.\n",
    "        \"\"\"\n",
    "        self.actions = (\"hit\", \"stick\") \n",
    "        self.lamb = lamb                                  # lambda parameter of the SARSA algorithm\n",
    "        self.n_episodes = n_episodes                      # number of episodes (games) to sample in order to make the agent learn\n",
    "        self.trace = trace\n",
    "        \n",
    "        self.features = self.create_features()                        # make features\n",
    "        self.n_features = len(self.features)\n",
    "        self.theta = self.init_theta()                                # init theta randomly\n",
    "        self.eligibilty_traces = np.zeros(self.n_features)            # init eligibilty traces to zeros\n",
    "                \n",
    "        self.alpha = lr\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Only used for plot in optimal policy\n",
    "        self.Q = self.init_to_zeros()\n",
    "\n",
    "        \n",
    "    def learn_q_value_function(self):  # TODO13\n",
    "        \"\"\"\n",
    "        Update the Q-function (i.e. the weights!) towards the target, and also the eligibility traces.\n",
    "        This function always considers the epsilon greedy policy.\n",
    "        Compute delta with the function below (to be implemented).\n",
    "        Both the updates of the weights and eligibility traces are taken into account in <update_step>. Use it!\n",
    "        \"\"\"\n",
    "        for i in range(self.n_episodes):\n",
    "            self.eligibilty_traces = np.zeros(self.n_features)            # init eligibilty traces to zeros\n",
    "            env = Jackblack()                                             # init a game sequence\n",
    "            state = env.state.copy()                                      # init state    \n",
    "            action = self.e_greedy_policy(state)                          # pick a first action\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "                \n",
    "    \n",
    "    def init_theta(self):\n",
    "        \"\"\"\n",
    "        Init the weights of the approximation function from a normal centered reduced gaussian distribution\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        theta : nd-array, random weights for the approximation\n",
    "        \"\"\"\n",
    "        \n",
    "        mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "        theta = np.random.normal(mu, sigma, self.n_features)\n",
    "        return theta\n",
    "\n",
    "    \n",
    "    def init_to_zeros(self):\n",
    "        \"\"\"\n",
    "        Used to init the Q-function and the incremental counter N at 0 for every state-action pairs.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        lookup_table : {state: (action)}, a dictionary of states as keys and dictionaries as value, in which the actions\n",
    "        are keys and corresponding Q-values are values\n",
    "        \"\"\"\n",
    "        dealer_scores = np.arange(1, 11)\n",
    "        player_scores = np.arange(1, 22)\n",
    "        states = [(dealer_score, player_score) for player_score in player_scores for dealer_score in dealer_scores]       \n",
    "        lookup_table = {}\n",
    "        for state in states:\n",
    "            lookup_table[state] = {\"hit\": 0, \"stick\": 0}  \n",
    "            \n",
    "        return lookup_table\n",
    "\n",
    "    \n",
    "    def create_features(self):\n",
    "        \"\"\"\n",
    "        Create the features for linear function approximation. Overlapping tile coding.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        features : list of tuples, each tuple is a combination of features (dealer_feature, player_feature, action)\n",
    "        \"\"\"\n",
    "        \n",
    "        dealer = [(1, 4), (4, 7), (7, 10)]\n",
    "        player = [(1, 6), (4, 9), (7, 12), (10, 15), (13, 18), (16, 21)]  # Overlapping features!\n",
    "        actions = [\"hit\", \"stick\"]\n",
    "        features = []\n",
    "        for d in dealer:\n",
    "            for p in player:\n",
    "                for a in actions:\n",
    "                    features.append((d, p, a))\n",
    "        return features\n",
    "\n",
    "    \n",
    "    def update_step(self, delta, state, action):  # TODO14\n",
    "        \"\"\"\n",
    "        Update the weights of the linear function towards the optimality, and also the eligibility traces. \n",
    "        According to the inner variable of the class, implement the correct version of the trace.\n",
    "        Here you need to accumulate or replace, but also to implement the \"fading\" mechanism!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : float, the TD error for the given state-action pair\n",
    "        state : dict, the current state\n",
    "        action : string, the current action\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        index_associated_features = []\n",
    "        for i, feature in enumerate(self.features):\n",
    "            if (feature[0][0] <= lookup_state[0] <= feature[0][1]) and \\\n",
    "               (feature[1][0] <= lookup_state[1] <= feature[1][1]) and \\\n",
    "               (action == feature[2]):                \n",
    "                index_associated_features.append(i)\n",
    "                \n",
    "        # Update eligibility traces here! Both accumulating and replacing version.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def compute_delta(self, state, action, next_state, next_action, reward):  # TODO15\n",
    "        \"\"\"\n",
    "        Compute delta, the TD error, with function approximation! You need to use phi (which is given)!\n",
    "        Recall the formula of the TD error under function approximation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, the current state\n",
    "        action : string, the current action\n",
    "        reward : int, the reward for the current state-action pair\n",
    "        next_state : dict, the state we end up in after performing the action\n",
    "        next_action : string, the action we take in next state following the policy\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        delta : float, the TD error\n",
    "        \"\"\"\n",
    "        lookup_state = tuple(state.values())\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "    def phi(self, state, action):\n",
    "        \"\"\"\n",
    "        The linear function phi, which compute the Q-value for a given state-action pair. Implements tile coding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple, the current state\n",
    "        action : string, the current action\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        q_value: float, the q value associated to a given state-action pair.\n",
    "        \n",
    "        \"\"\"\n",
    "        index_associated_features = []\n",
    "        for i, feature in enumerate(self.features):\n",
    "            if (feature[0][0] <= state[0] <= feature[0][1]) and \\\n",
    "               (feature[1][0] <= state[1] <= feature[1][1]) and \\\n",
    "               (action == feature[2]):\n",
    "                index_associated_features.append(i)\n",
    "        q_value = np.sum(np.take(self.theta, index_associated_features, axis=0))\n",
    "        \n",
    "        return q_value   \n",
    "    \n",
    "    \n",
    "    def random_policy(self):  # TODO16\n",
    "        \"\"\"\n",
    "        Return an action following a random policy (clearly state free).\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, random action\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def e_greedy_policy(self, state):  # TODO17\n",
    "        \"\"\"\n",
    "        Return an action given an epsilon greedy policy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state where we pick the action\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, action from epsilon greedy policy\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def get_action_w_max_value(self, state):\n",
    "        \"\"\"\n",
    "        Return the action with the max Q-value at a given state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : dict, state for the Q-function\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        action : string, best action for the state according to the approximated Q-function\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lookup_state = tuple(state.values())\n",
    "        except AttributeError:\n",
    "            lookup_state = state\n",
    "        if self.phi(lookup_state, \"hit\") == self.phi(lookup_state, \"stick\"):\n",
    "            return self.random_policy()\n",
    "        else:\n",
    "            if self.phi(lookup_state, \"hit\") > self.phi(lookup_state, \"stick\"):\n",
    "                return \"hit\"\n",
    "            else:\n",
    "                return \"stick\"\n",
    "        \n",
    "\n",
    "    def optimal_policy(self):\n",
    "        \"\"\"\n",
    "        Return the learned value-function and a dataframe with the optimal action for each state.\n",
    "        Note the use of phi!\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        max_Q : nd-array, the value-function\n",
    "        df_pivot : dataframe, representing the best action for each state\n",
    "        \"\"\"\n",
    "        max_Q = np.ndarray(shape=(21, 10))\n",
    "        df = pd.DataFrame(columns=[\"dealer_showing\", \"player_score\", \"best_action\"])\n",
    "        states = list(self.Q.keys())\n",
    "        for i in range(len(states)):\n",
    "            best_action = self.get_action_w_max_value(states[i])\n",
    "            max_Q[states[i][1]-1][states[i][0]-1] = self.phi(states[i], best_action)\n",
    "            df.loc[i] = (states[i][0], states[i][1], best_action)\n",
    "        df_pivot = df.pivot(\"player_score\", \"dealer_showing\", \"best_action\")\n",
    "        \n",
    "        return max_Q, df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f07101473152dde5034200b8670631d",
     "grade": true,
     "grade_id": "cell-660dd70935541338",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AT THIS POINT THE SARSA WITH APPROXIMATION CLASS MUST BE COMPLETED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b995e1c828e56b530416a38752265a3c",
     "grade": false,
     "grade_id": "cell-2c48ef7ec04fa4a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's play 50_000 episodes and learn the approximated Q-function with the SARSA algorithm with function approximation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T09:39:04.749349Z",
     "start_time": "2020-06-21T09:39:04.732376Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56da74632b1e2a697e8b4ad7613a7b3",
     "grade": false,
     "grade_id": "cell-d6186eafabc988b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "va = Value_Approximation(n_episodes=50_000)\n",
    "va.learn_q_value_function()\n",
    "vavalue, vapol = va.optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79c16f61dbaa3edd638704c765043ffb",
     "grade": false,
     "grade_id": "cell-f0ac5fd37e009cbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Display the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f4e8f0784ac43864d88d3991621a670",
     "grade": false,
     "grade_id": "cell-5daa21a01bb1eced",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "display(vapol)\n",
    "plot_table(vavalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2a6a979df3aa85b2f40dd3b0a5a5ae4",
     "grade": false,
     "grade_id": "cell-e9c8cf2b52cf4947",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TODO18\n",
    "Comment the results you obtained with the three different procedures with regard to the optimal policy, the value-function and the computational burden. Do not describe the results, but try to say meaningful things about _why_ you obtained such results.\n",
    "You do not need more than 7-10 lines.\n",
    "\n",
    "Answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab4f9af8d29c2f22df425fb74255f562",
     "grade": true,
     "grade_id": "cell-e7e2799e79b00e1d",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

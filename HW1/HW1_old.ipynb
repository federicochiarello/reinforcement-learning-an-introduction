{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Homework\n",
    "\n",
    "Before you turn this notebook in, make sure everything runs as expected. First, **restart the kernel** (in the menu bar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menu bar, select Cell $\\rightarrow$ Run All). You can also perform the two operations in one step (in the menu bar, select Kernel $\\rightarrow$ Restart & Run All).\n",
    "\n",
    "Make sure you fill in any place (or anywhere that you want in the code) that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", and remove every line containing the expression: \"raise ...\" (if you leave such a line your code will not run). This is a reminder for you to complete all parts of the notebook. You can also send an incomplete notebook, but please remove all 'raise'-lines in any case.\n",
    "\n",
    "You are not required to modify the code only where the line `YOUR CODE HERE` appears. In fact you can modify the given function and code as you want. Those are just reminders.\n",
    "\n",
    "Do not remove any cell from the notebook you downloaded. You can add any number of cells (and remove them if not more necessary). \n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your code, since this is the way we will do it before evaluating your notebook!\n",
    "\n",
    "Fill in your name and ID number (matricola) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = ''\n",
    "ID_number = int('')\n",
    "\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b1badaa6319b679cb73badf76fb53b2",
     "grade": false,
     "grade_id": "cell-366b9978c3dfec12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# First Homework - Multi-Armed Bandits and Dynamic Programming\n",
    "\n",
    "The present homework contains 2 exercises, one related to Multi-Armed Bandit problem and the other to Dynamic Programming.\n",
    "\n",
    "- Delivery date: 26/10/2022\n",
    "\n",
    "- Submission deadline: 09/11/2022 (23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fed4e2b3068240653b0ef1408390e9d",
     "grade": false,
     "grade_id": "cell-4115c6a630376d7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Nonstationary Multi-Armed Bandits\n",
    "\n",
    "The first part of the homework deals with the Multi Armed Bandit Problem.\n",
    "In particular, the 10-armed testbed environment is exploited in order to understand what happens when dynamics are introduced in the arm rewards.\n",
    "In order to complete the following exercise you will have to **modify** the 10-armed testbed environment in such a way that the mean values $q(a)$ of all the arms **start out equal** and then take **independent random walks**. This means that we will have:\n",
    "\n",
    "$$ q_0(a) = 0, \\qquad \\forall a \\in \\mathcal{A}$$\n",
    "\n",
    "$$q_t(a) = q_{t-1}(a) + w_{t-1}, \\qquad \\forall a \\in \\mathcal{A}, \\qquad \\textrm{where} \\quad w_t \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Assume $\\sigma = 0.1$.\n",
    "\n",
    "Compare the performance of two different instances of the **Epsilon-Greedy algorithm**, one using **sample averages**, and the other using a **constant step size** $\\alpha = 0.1$.\n",
    "Plot the **average reward**, the **cumulative average reward**, and the **average percentage of optimal actions**: use $\\epsilon=0.1$ and average over $500 \\ \\texttt{runs}$ for $5000 \\ \\texttt{steps}$.\n",
    "\n",
    "##  IMPORTANT: Make sure that **before** the cell saying 'HERE ENDS THE FIRST EXERCISE' you have a variable called 'rewards' which is a numpy nd-array containing the average (over different runs) rewards of the two algorithms! To check this, you can run the assert statement in the cell: if it does not raise an error, you should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages in ipython\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1ddab9c900106cbff4f31a83e121879",
     "grade": false,
     "grade_id": "cell-b4bbef8d70c1b0c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Credits and imports\n",
    "\n",
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Artem Oboturov(oboturov@gmail.com)                             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5a81f4af977d6f4130f0399f093cb36",
     "grade": false,
     "grade_id": "cell-40ccc5e870a6c130",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For your convenience, the code discussed in the first \"tutorial lesson\" is reported below. Feel free to modify the code as you wish in order to achieve the required result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5abf678e11035189e9ae213c7f1e0b",
     "grade": false,
     "grade_id": "cell-9445d2885b226ef8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    # @k_arm: # of arms\n",
    "    # @epsilon: probability for exploration in epsilon-greedy algorithm\n",
    "    # @initial: initial estimation for each action\n",
    "    # @step_size: constant step size for updating estimations\n",
    "    # @sample_averages: if True, use sample averages to update estimations instead of constant step size\n",
    "    # @UCB_param: if not None, use UCB algorithm to select action\n",
    "    # @gradient: if True, use gradient based bandit algorithm\n",
    "    # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
    "    def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=False, UCB_param=None,\n",
    "                 gradient=False, gradient_baseline=False, true_reward=0.):\n",
    "        self.k = k_arm\n",
    "        self.step_size = step_size\n",
    "        self.sample_averages = sample_averages\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.UCB_param = UCB_param\n",
    "        self.gradient = gradient\n",
    "        self.gradient_baseline = gradient_baseline\n",
    "        self.average_reward = 0\n",
    "        self.true_reward = true_reward\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "\n",
    "    # get an action for this bandit\n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices) \n",
    "\n",
    "        if self.UCB_param is not None:\n",
    "            UCB_estimation = self.q_estimation + \\\n",
    "                self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5))\n",
    "            q_best = np.max(UCB_estimation)\n",
    "            return np.random.choice(np.where(UCB_estimation == q_best)[0])\n",
    "\n",
    "        if self.gradient:\n",
    "            exp_est = np.exp(self.q_estimation)\n",
    "            self.action_prob = exp_est / np.sum(exp_est)\n",
    "            return np.random.choice(self.indices, p=self.action_prob)\n",
    "\n",
    "        q_best = np.max(self.q_estimation)\n",
    "        return np.random.choice(np.where(self.q_estimation == q_best)[0])\n",
    "\n",
    "#     # take an action, update estimation for this action\n",
    "#     def step(self, action):\n",
    "#         return reward\n",
    "\n",
    "#     def reset(self):\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def simulate(runs, time, bandits):\n",
    "    rewards = np.zeros((len(bandits), runs, time))\n",
    "    best_action_counts = np.zeros(rewards.shape)\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        for r in trange(runs):\n",
    "            bandit.reset()\n",
    "            for t in range(time):\n",
    "                action = bandit.act()\n",
    "                reward = bandit.step(action)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "    mean_best_action_counts = best_action_counts.mean(axis=1)\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    return mean_best_action_counts, mean_rewards\n",
    "\n",
    "# Recall you have to modify the environment above!\n",
    "\n",
    "# Code for the figures\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a821c1326e74fe4b5a1f21dab6b3179",
     "grade": true,
     "grade_id": "cell-0889dcc45d077de2",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HERE ENDS THE FIRST EXERCISE\n",
    "assert rewards.shape == (2, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c90b392540d72de82b8b7fab64d3dcee",
     "grade": false,
     "grade_id": "cell-870d2160c5e5e938",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Comment your results in the cell below. Which algorithm performed best? With respsect to which metric? Why? [~15 lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f9ce0706a0eed49f28a7538478c8922",
     "grade": true,
     "grade_id": "cell-0c1eb10da17b5c2f",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d22567656a0695459ab75d3cd7c984a5",
     "grade": false,
     "grade_id": "cell-ecab71c755fdcf6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Jack against the nationwide company\n",
    "\n",
    "The second part of the homework consists of a modification of the Jack's Car Rental problem discussed during the second \"Tutorial Lecture\".\n",
    "One of Jackâ€™s employees at the first location rides a bus home each night and lives near the second location. She is happy to **shuttle one car to the second location for free**, while each additional car costs the usual $2\\$$. Moreover, the nationwide car rental company has changed its policy on parking spaces: for each location now Jack can only park up to 10 vehicles free of charge. **If more than 10 cars are kept overnight at a location, then an additional cost of $4\\$$ must be incurred** to use a second parking lot (independent of how many cars are kept there). The car rental company charges Jack with the additional parking lots price first thing in the morning, **before any possible moving of cars**.\n",
    "In order to fulfill the exercise requirements, you need to **modify and solve** the new instance of the Jack's car rental problem with a **Value Iteration** algorithm.\n",
    "Generate a subplot with six cells as the one seen in the \"Tutorial Lecture\" with the figures of 5 policies at different iterations: #0, #7, #14, #21, and the last (optimal!) one. In the sixth box plot the optimal value-function.\n",
    "Use $\\texttt{constant}$_$\\texttt{returned}$_$\\texttt{cars}$ = **True**.\n",
    "\n",
    "**Optional assignment:** \n",
    "consider an additional cost, when the customers return more cars than a facility is able to accomodate (MAX_CARS) Jack has to pay a tow truck to carry them to a third location of the nationwide company. Suppose that the truck can carry all the extra cars, and it costs 4 \\$ for a single location. Be careful of where you encode this in the script. \n",
    "\n",
    "If you decide to implement this extra point, make sure to set $\\text{OPTIONAL}$_$\\text{POINT}$ = **True**, otherwise leave it to **False**.\n",
    "\n",
    "\n",
    "##  IMPORTANT: Make sure that **before** the cell saying 'HERE ENDS THE FIRST EXERCISE' you have a variable called 'policy' which is a numpy nd-array containing the optimal action for each pair of states, as in the notebook of the second \"Tutorial Lecture\"! To check this, you can run the assert statement in the cell: if it does not raise an error, you should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b44ccf5666ef19babeac28631d597ece",
     "grade": false,
     "grade_id": "cell-fb7d0696967f4e1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# maximum # of cars in each location\n",
    "MAX_CARS = 20\n",
    "\n",
    "# maximum # of cars to move during night\n",
    "MAX_MOVE_OF_CARS = 5\n",
    "\n",
    "# expectation for rental requests in first location\n",
    "RENTAL_REQUEST_FIRST_LOC = 3\n",
    "\n",
    "# expectation for rental requests in second location\n",
    "RENTAL_REQUEST_SECOND_LOC = 4\n",
    "\n",
    "# expectation for # of cars returned in first location\n",
    "RETURNS_FIRST_LOC = 3\n",
    "\n",
    "# expectation for # of cars returned in second location\n",
    "RETURNS_SECOND_LOC = 2\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# credit earned by a car\n",
    "RENTAL_CREDIT = 10\n",
    "\n",
    "# cost of moving a car\n",
    "MOVE_CAR_COST = 2\n",
    "\n",
    "# all possible actions\n",
    "actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)\n",
    "\n",
    "# An upper bound for poisson distribution\n",
    "# If n is greater than this value, then the probability of getting n is truncated to 0\n",
    "POISSON_UPPER_BOUND = 11\n",
    "\n",
    "# Probability for poisson distribution\n",
    "# @lam: lambda should be less than 10 for this function\n",
    "poisson_cache = dict()\n",
    "\n",
    "def poisson_probability(n, lam):\n",
    "    global poisson_cache\n",
    "    key = n * 10 + lam\n",
    "    if key not in poisson_cache:\n",
    "        poisson_cache[key] = poisson.pmf(n, lam)\n",
    "    return poisson_cache[key]\n",
    "\n",
    "def expected_return(state, action, state_value, constant_returned_cars):\n",
    "    \"\"\"\n",
    "    @state: [# of cars in first location, # of cars in second location]\n",
    "    @action: positive if moving cars from first location to second location,\n",
    "            negative if moving cars from second location to first location\n",
    "    @stateValue: state value matrix\n",
    "    @constant_returned_cars:  if set True, model is simplified such that\n",
    "    the # of cars returned in daytime becomes constant\n",
    "    rather than a random value from poisson distribution, which will reduce calculation time\n",
    "    and leave the optimal policy/value state matrix almost the same\n",
    "    \"\"\"\n",
    "    # initialize total return\n",
    "    returns = 0.0\n",
    "    \n",
    "    # cost for moving cars\n",
    "    returns -= MOVE_CAR_COST * abs(action)\n",
    "    \n",
    "    # Recall you have to modify the environment below!\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # moving cars\n",
    "    NUM_OF_CARS_FIRST_LOC = min(state[0] - action, MAX_CARS)\n",
    "    NUM_OF_CARS_SECOND_LOC = min(state[1] + action, MAX_CARS)\n",
    "\n",
    "    # go through all possible rental requests\n",
    "    for rental_request_first_loc in range(POISSON_UPPER_BOUND):\n",
    "        for rental_request_second_loc in range(POISSON_UPPER_BOUND):\n",
    "            # probability for current combination of rental requests\n",
    "            prob = poisson_probability(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \\\n",
    "                poisson_probability(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC)\n",
    "\n",
    "            num_of_cars_first_loc = NUM_OF_CARS_FIRST_LOC\n",
    "            num_of_cars_second_loc = NUM_OF_CARS_SECOND_LOC\n",
    "\n",
    "            # valid rental requests should be less than actual # of cars\n",
    "            valid_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc)\n",
    "            valid_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc)\n",
    "\n",
    "            # get credits for renting\n",
    "            reward = (valid_rental_first_loc + valid_rental_second_loc) * RENTAL_CREDIT\n",
    "            num_of_cars_first_loc -= valid_rental_first_loc\n",
    "            num_of_cars_second_loc -= valid_rental_second_loc\n",
    "\n",
    "            if constant_returned_cars:\n",
    "                # get returned cars, those cars can be used for renting tomorrow\n",
    "                returned_cars_first_loc = RETURNS_FIRST_LOC\n",
    "                returned_cars_second_loc = RETURNS_SECOND_LOC\n",
    "                num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc])\n",
    "            else:\n",
    "                for returned_cars_first_loc in range(POISSON_UPPER_BOUND):\n",
    "                    for returned_cars_second_loc in range(POISSON_UPPER_BOUND):\n",
    "                        prob_return = poisson_probability(\n",
    "                            returned_cars_first_loc, RETURNS_FIRST_LOC) * poisson_probability(returned_cars_second_loc, RETURNS_SECOND_LOC)\n",
    "                        num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                        num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                        prob_ = prob_return * prob\n",
    "                        returns += prob_ * (reward + DISCOUNT *\n",
    "                                            state_value[num_of_cars_first_loc_, num_of_cars_second_loc_])\n",
    "    return returns\n",
    "\n",
    "# This is the parameter of the function in the original code\n",
    "constant_returned_cars = True\n",
    "\n",
    "value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "\n",
    "policy = np.zeros(value.shape, dtype=np.int)\n",
    "\n",
    "iterations = 0\n",
    "_, axes = plt.subplots(2, 3, figsize=(40, 20))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Code for value iteration algorithm and figures\n",
    "OPTIONAL_POINT = False\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd67d63b3d5bfba9178f844b01c60910",
     "grade": true,
     "grade_id": "cell-d8e8818d8b45e92d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HERE ENDS THE SECOND EXERCISE\n",
    "assert policy.shape == (21, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68a5e4a9895070a42375e8eca9f76a45",
     "grade": false,
     "grade_id": "cell-16bd3c78ce56d20b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Comment your results in the cell below. Can you give an intuitive explanation for the optimal policy you found? [~15 lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9b332c365f4de6f6b74ba9e60f0f176",
     "grade": true,
     "grade_id": "cell-8c36c5f8c413e830",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
